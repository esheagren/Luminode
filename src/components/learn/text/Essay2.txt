## 2. Exploring Vector Embeddings Technically

After building some intuition, we can dive into the technical details of how embeddings are created and used. Modern embedding techniques have evolved from simple counts to complex neural models. In *Luminode*, the embeddings come from a large language model (LLaMA), stored and indexed in a Pinecone vector database for fast search. Let’s unpack these components step by step.

### 2.1 Mathematical Foundations of Embeddings

**Distributional semantics:** The core idea that underpins many embedding techniques is that words used in similar **contexts** have similar meanings. This is operationalized by collecting statistics from a large text corpus. Early approaches like Latent Semantic Analysis (LSA) in the 1990s built large co-occurrence matrices (rows = words, columns = context features) and then factorized them (e.g. via Singular Value Decomposition) to find a lower-dimensional representation capturing the gist of those co-occurrence patterns ([Word embedding - Wikipedia](https://en.wikipedia.org/wiki/Word_embedding#:~:text=phrases%20from%20the%20vocabulary%20are,vectors%20%20of%20%20207)). The lower-dimensional “latent semantic space” from LSA was an early form of word embeddings.

**Word2Vec (Mikolov et al., 2013):** A breakthrough in efficiency and quality came with Word2Vec ([Word2vec - Wikipedia](https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20is%20a%20technique%20in,2)). Instead of explicit matrix factorization, Word2Vec trains a simple neural network to predict context words from a given word (Skip-gram model) or vice versa (CBOW). In doing so, it learns vector representations that maximize the probability of real context words and minimize that of random words. After training on a large corpus, each word is associated with a dense vector (often 100–300 dimensions) such that **words appearing in similar contexts have nearby vectors** ([Word2vec - Wikipedia](https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20represents%20a%20word%20as,Germany)). These vectors were remarkably good at capturing semantics: Word2Vec famously demonstrated the **analogy** capability described above, and could group synonyms and related concepts via simple vector math. In fact, once trained, a Word2Vec model can *“detect synonymous words”* or suggest related words by finding nearest vectors ([Word2vec - Wikipedia](https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20is%20a%20technique%20in,2)). The success of Word2Vec also showed the advantage of **dense, continuous vectors** over earlier sparse representations – it generalizes better and encodes subtle gradations of meaning. (Another contemporary model, **GloVe** (Pennington et al., 2014), took a complementary approach by factorizing a word co-occurrence matrix with a clever weighting scheme to also produce embeddings capturing global corpus statistics.)

**Transformer-based embeddings:** As of the late 2010s, static word vectors like those from Word2Vec/GloVe were surpassed by **contextual embeddings** from large Transformer models ([Word2vec - Wikipedia](https://en.wikipedia.org/wiki/Word2vec#:~:text=As%20of%202022%2C%20the%20straight,11)). Models like ELMo (2018), BERT (2018) and later GPT-series and LLaMA (2023) produce embeddings *on the fly* for each word *in context*. For example, **“bank”** in *“river bank”* vs *“bank account”* will have different vectors under BERT, reflecting the different senses. These models are trained with objectives like masked language modeling, forcing them to develop rich vector representations in their deeper layers to predict missing words. The result is that the *internal* embeddings (e.g. the output of BERT’s final layer for a given token or sentence) encode a great deal of semantic and syntactic information useful for NLP tasks. In practice, one can take a large pre-trained transformer and use it as an **embedding generator** – feeding in text and extracting the hidden state vector. For instance, LLaMA (a family of large language models by Meta AI) has an embedding dimension of around 4,000 (for LLaMA-2 7B model) for each token ([Getting word embeddings for llama2 | by Alexandre Sauquet | Medium](https://medium.com/@sauquetalex/getting-word-embeddings-for-llama2-9a01c0d07d37#:~:text=Tokens%20can%20be%20converted%20to,use%20llama2%20to%20embed%20words)). Those high-dimensional vectors can be used to represent the meaning of words or entire sentences. *Luminode* uses **LLaMA-based embeddings** for its vocabulary of ~50,000 words, meaning each word is mapped to a 1000-dimensional vector (the LLaMA vectors may have been reduced in dimensionality or truncated to 1000 numbers for efficiency). These vectors come from LLaMA’s learned representation of language, a state-of-the-art source of semantic embeddings as of 2023.

In summary, whether via Word2Vec, GloVe, or Transformers, the goal is to produce a **distributed vector representation** for each item such that meaningful relations are preserved. The improvements over time (from static to contextual embeddings) mean modern systems capture nuance like polysemy and context-specific meaning. But at their core, all these methods rely on training signals that bring similar meanings closer together in the vector space and distinguish dissimilar ones.

### 2.2 Cosine Similarity, Distances, and Neighbors

Once we have vectors representing data (words, sentences, images, etc.), we need ways to **compare** them. A common choice is **cosine similarity**, which measures the cosine of the angle between two vectors. Two vectors pointing in very similar directions (small angle) have cosine similarity near 1.0, whereas orthogonal or opposite vectors have similarity near 0 or -1. Cosine similarity effectively ignores the magnitude of vectors and focuses on orientation – this is useful because in many embedding schemes the magnitude may not be as meaningful as the direction of the vector. Word2Vec and most other text embeddings use cosine (or equivalently, *dot product* after normalizing vectors) as the similarity metric ([Word2vec - Wikipedia](https://en.wikipedia.org/wiki/Word2vec#:~:text=Word2vec%20represents%20a%20word%20as,Germany)). If vectors are normalized to unit length, nearest neighbors under Euclidean distance are the same as nearest by cosine. In *Luminode*, when you click *“Measure”* or compute distances, it’s using cosine similarity to quantify how related two words are in the embedding space (e.g. *cat* might have cosine ~0.8 with *dog*, higher than with an unrelated word like *banana*).

Finding the **nearest neighbors** of a vector is a fundamental operation on embeddings. Given a set of many vectors (e.g. the 50k words in the Luminode database), the nearest neighbor search finds the top *k* vectors that are most similar to a query vector. This powers things like *“find similar words”* or semantic search. A naive approach would compare the query to every vector in the set and pick the closest, but that becomes computationally expensive as the dataset grows. Instead, specialized data structures and algorithms are used to speed this up. This task – retrieving most similar vectors – is known as **vector search** or **nearest neighbor search** ([What is Similarity Search? | Pinecone](https://www.pinecone.io/learn/what-is-similarity-search/#:~:text=This%20is%20where%20the%20similarity,this%20task%20nearest%20neighbor%20search)). Many libraries and services (FAISS, Annoy, ScaNN, etc.) implement **Approximate Nearest Neighbor (ANN)** methods that trade a tiny bit of accuracy for huge gains in speed ([What is Similarity Search? | Pinecone](https://www.pinecone.io/learn/what-is-similarity-search/#:~:text=Approximate%20Neighbor%20Search)). They might organize vectors in a graph or tree structure so that the search can skip large portions of the data and quickly home in on the likely nearest points ([What is Similarity Search? | Pinecone](https://www.pinecone.io/learn/what-is-similarity-search/#:~:text=In%20approximately%20nearest%20neighbors%20,actually%20dive%20into%20how%20approximate)).

In *Luminode*, the back-end uses **Pinecone**, a managed vector database, to handle similarity search. Pinecone is optimized for exactly this purpose: it stores the high-dimensional vectors and can return the nearest matches in milliseconds even if there are millions of vectors indexed ([What is Similarity Search? | Pinecone](https://www.pinecone.io/learn/what-is-similarity-search/#:~:text=Pinecone%C2%A0is%20a%20vector%20database%20that,makes%20similarity%20search%20so%20useful)). Under the hood, Pinecone likely uses ANN algorithms (such as HNSW – Hierarchical Navigable Small World graphs) to index embeddings for fast retrieval. So when Luminode finds the nearest words for *“cat”* or calculates the analogy *“king – man + woman”*, Pinecone is doing the heavy lifting of vector math and search in the background. According to Pinecone’s documentation, vector databases enable working with semantic representations at scale, where the *distance between vectors reflects the similarity of the objects* they represent ([What is Similarity Search? | Pinecone](https://www.pinecone.io/learn/what-is-similarity-search/#:~:text=vector%20embeddings,us%20into%20a%20vector%20space)). Each query returns items with a similarity score (like cosine similarity) indicating how close they are to the query ([Query data - Pinecone Docs](https://docs.pinecone.io/guides/data/query-data#:~:text=Query%20data%20,similarity%20metric%20for%20the%20index)). This infrastructure allows interactive tools like Luminode to rapidly display nearest neighbors or analogy results from a large vocabulary.

### 2.3 Dimensionality Reduction and Visualization (PCA)

One challenge with embeddings is that we can’t directly *see* a 1000-dimensional space. To visualize or interpret the structure, we often apply **dimensionality reduction** techniques. These create a low-dimensional projection (usually 2D or 3D) that preserves as much of the original relationships as possible ([Dimensionality reduction - Wikipedia](https://en.wikipedia.org/wiki/Dimensionality_reduction#:~:text=Dimensionality%20reduction%2C%20or%20dimension%20reduction%2C,signal%20processing%20%2C%20%2073)). One simple and popular linear method is **Principal Component Analysis (PCA)**. PCA finds the directions (principal components) of greatest variance in the high-dimensional data and projects the data onto a few of those directions. By taking, say, the top 2 principal components of the word vectors, we can plot each word in a 2D plane. Words that were near each other in 1000D will typically still end up near each other in the 2D PCA plot, to the extent possible. Non-linear techniques like **t-SNE** or **UMAP** are also widely used, as they can sometimes capture local neighborhoods more faithfully (at the expense of distorting global structure). These methods learn a mapping such that if two points were close in the original high-D space, their 2D projections will also be close ([Visualising Embeddings with t-SNE | by Nadine Amersi-Belton | Analytics Vidhya | Medium](https://medium.com/analytics-vidhya/visualising-embeddings-with-t-sne-b54bf6b635f#:~:text=We%20will%20now%20use%20the,be%20close%20to%20each%20other)).

*Luminode*’s **graphing** feature takes advantage of such projections to display words on a plane (or in 3D). As you add words to the workspace, each is plotted as a point. Underneath, the system has reduced the original LLaMA embedding vectors down to 2 or 3 dimensions (likely using PCA on the full set of embeddings, or possibly a learned projection). The result is that you can *see* semantic structure: clusters form where related words congregate, and unrelated words are far apart in the plot. For example, if you add “cat”, “dog”, “canine”, “kitty” to Luminode, you’ll notice they appear in a tight cluster, reflecting their semantic similarity (all related to small domesticated animals) ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=idea%2C%20so%20here%27s%20what%20it,Are%20closer%20to%20each%20other)) ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=relationships%20between%20different%20words%20that%27s,of%20these%20words%20that%20are)). Meanwhile, if you also add “king” and “queen”, those will appear in a different region of the plot, likely closer to other human or royal terms. Luminode even allows toggling into a 3D view, adding an extra axis to reveal more structure ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=to%20represent%20each%20of%20these,can%20actually%20be%20described%20very)). This is useful because sometimes a 2D projection squeezes data in a way that overlaps points that are actually separate in higher dimensions – the 3D view can disentangle some of that by giving an additional degree of freedom.

In short, dimensionality reduction helps bridge the gap between human intuition and high-dimensional math. It *“retains meaningful properties of the original data”* in a low-dimensional representation ([Dimensionality reduction - Wikipedia](https://en.wikipedia.org/wiki/Dimensionality_reduction#:~:text=Dimensionality%20reduction%2C%20or%20dimension%20reduction%2C,signal%20processing%20%2C%20%2073)), allowing us to visually inspect the “shape” of meaning. Tools like Luminode use this to great effect, turning abstract vectors into intuitive maps where you can directly observe the geometry of meaning we discussed in Section 1.

### 2.4 Exploring Luminode’s Interface: Search, Analogies, and the “Slice” Method

With a grasp of the underlying embeddings and similarity search, we can appreciate the interactive features Luminode offers:

- **Semantic search & nearest neighbors:** In Luminode, you can type in a word and the system finds the nearest words in the database. For instance, querying **“eagle”** might retrieve *hawk, falcon, bird, vultures*, etc. – terms that lie close to *eagle* in the vector space ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=just%20clicked%20on%20this%20and,the%20semantic%20relationships%20between%20different)). This is essentially a nearest neighbor search powered by Pinecone. The returned words are those with highest cosine similarity to the query. Because the embedding space groups words by meaning, this functions like a thesaurus or semantic search engine. (It can also reveal some surprising neighbors: since the embeddings capture statistical usage, occasionally a word will be near another not obviously related – these artifacts reflect the oddities of how words co-occur in the source corpus, giving the space an *“alien”* quality at times ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=in%20here%20that%20seem%20not,So%20we%20can)).)
- **Distance measurements:** The interface can draw lines or display distances between points, which concretely shows the **cosine similarity** values. Selecting a set of words and enabling *“measure”* will visualize the network of pairwise similarities ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=relate%20to%20one%20another,of%20these%20words%20that%20arises)). For example, if you have *cat, dog, banana* plotted, you might see lines connecting *cat–dog* (short line for high similarity) versus *cat–banana* (no line or a much longer one for low similarity). This feature reinforces the notion that the layout isn’t arbitrary – distances correspond to semantic closeness that can be measured precisely.
- **Analogy tool:** As demonstrated, Luminode lets you ask analogies like *“King is to Queen as Man is to ___?”* and will compute the answer by vector arithmetic. It takes the vectors (king – man + woman) and then finds the nearest neighbor to the resulting vector – ideally **“queen”** ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=just%20kind%20of%20find%20closest,Oops%2C%20America%20and)). In practice, the top result is often the expected answer (queen for the above analogy), though not always exact due to noise and finite vocabulary. Another example given was *“Paris is to France as Tokyo is to ___?”* which should yield **“Japan.”** This tool showcases the linear relationships present in the space. It’s essentially automating what we described earlier: searching for a word whose embedding is closest to a specified linear combination of other embeddings ([King - man + woman = queen: the hidden algebraic structure of words | School of Informatics](https://informatics.ed.ac.uk/news-events/news/news-archive/king-man-woman-queen-the-hidden-algebraic-struct#:~:text=is%20that%C2%A0analogies%2C%20e.g.%20,Of%20course%2C%20this%20result)).
- **Slice (vector interpolation) method:** *Slice* is a novel feature contributed by Luminode’s creator ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=these%20are%20just%20like%20very,can%20see%20what%20intermediate%20concepts)). It lets you pick two words and then it **interpolates** between their vectors, showing which words lie along the straight line connecting the two concepts. Mathematically, it finds mid-points in the vector space between the two endpoints (in a binary search fashion: first the direct midpoint, then midpoints between that and each end, etc.) ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=you%20is%20it%27s%20basically%20what,Golds)). At each of those intermediate points, it finds the nearest actual word in the database. The effect is that you traverse a path through the semantic space and see which words pop up as you go. For example, between **“gold”** and **“copper,”** the slice might show intermediate words like *silver*, *iron*, *metallic* – terms related to metals and elements ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=cool,And)). Between very unrelated words (say “puppy” and “it”), the path might wander through some tangentially related terms (*puppy → dog → ... → it*) ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=like%20why%20those%20are%20the,there%27s%20a%20nice%20intermediate%20space)). The slice tool is fascinating as an exploration of the *continuity* of the embedding space – it treats meaning as a spectrum between concepts. While not every intermediate point is interpretable (the path can sometimes seem random if the concepts are too far apart or the space has odd curvature), it provides insight into how one concept can morph into another via gradual semantic shifts. In essence, it asks: “If we gradually change this concept into that one, what are the concepts we pass along the way?”
- **Graphing and clustering:** As mentioned, Luminode plots the selected words in 2D or 3D. This visual clustering helps users see patterns. For example, if you input a bunch of animal names and a bunch of country names, you’d likely see two distinct clusters on the graph – one for animals and one for countries. You might also notice substructure: within animals, maybe domestic vs wild animals cluster slightly apart, etc. This mirrors what unsupervised clustering algorithms would find, but it’s done visually. You can literally hover over a word and see its numeric vector (truncated) as well ([Over the last two months, I have been feverishly building what I imagine… | Erik Sheagren](https://www.linkedin.com/posts/erik-sheagren-40800a179_over-the-last-two-months-i-have-been-feverishly-activity-7312962008109174785-fcNo#:~:text=words,of%20these%20words%20that%20are)), reinforcing that each point corresponds to a high-dimensional coordinate.

All these interface features – search, distances, analogies, slicing, plotting – are different ways of *interrogating the embedding space*. They help build intuition about how meaning is stored. For instance, after playing with several examples, you might conclude that the **direction** from *“man”* to *“woman”* in the space seems to consistently correspond to a gender swap (try *“brother” → “sister”*, *“father” → “mother”*, they will likely align similarly). Or you might find a distinct cluster of tech terms separate from a cluster of food terms, indicating the model differentiates those domains well. In sum, Luminode provides an interactive window into the otherwise invisible high-dimensional world of embeddings.